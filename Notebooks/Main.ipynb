{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389db85d-4933-4281-8baa-0b65d30e38fa",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930c4656-e9ec-4f43-aa06-667648266285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from skimage import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, classification_report, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Activation, Flatten, Dense, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.insert(1, '../Src/Lib')\n",
    "from functions import image_augmentation, image_preprocess\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe46d53-b382-46da-a8b7-ac56770f8045",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4d1b5f5-6732-4884-904c-c05d900ab147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is divided in 4 files, this concatenates them all\n",
    "\n",
    "df_list = []\n",
    "for file_name in glob.glob(\"../Data/Raw/Archive/*.txt\"):\n",
    "    df_temp = pd.read_csv(file_name, sep=\"\\t\")\n",
    "    df_list.append(df_temp)\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a739f-adc5-478f-a6d8-bd6317471468",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ede779-2b17-4da1-86ff-db4fa0c52731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of missing values\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[df[\"age\"] != \"None\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42819457-691f-4945-a6a7-a2a46ff3c60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 32)     4953\n",
       "(0, 2)       2488\n",
       "(38, 43)     2293\n",
       "(4, 6)       2140\n",
       "(8, 12)      2119\n",
       "(15, 20)     1642\n",
       "(60, 100)     867\n",
       "(48, 53)      825\n",
       "35            293\n",
       "13            168\n",
       "22            149\n",
       "34            105\n",
       "23             96\n",
       "45             88\n",
       "(27, 32)       77\n",
       "55             76\n",
       "36             56\n",
       "(38, 42)       46\n",
       "57             24\n",
       "3              18\n",
       "29             11\n",
       "(38, 48)        6\n",
       "58              5\n",
       "2               3\n",
       "(8, 23)         1\n",
       "42              1\n",
       "46              1\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This dataset is meant to make the age a classification problem, but we are going to make it a regression one\n",
    "\n",
    "df[\"age\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb05e6c-4f03-401a-89bb-a678f01c1b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating two lists, one with the current values (keys) and one with the new values (values), latter being the average of the first.\n",
    "\n",
    "ages_keys = df[\"age\"].value_counts().index\n",
    "ages_values = []\n",
    "for x in df[\"age\"].value_counts().index:\n",
    "    if x.startswith(\"(\"):\n",
    "        x = x.split(\", \")\n",
    "        x[0] = x[0].replace(\"(\",\"\")\n",
    "        x[1] = x[1].replace(\")\",\"\")\n",
    "        x[0] = int(x[0])\n",
    "        x[1] = int(x[1])\n",
    "        x = int((x[0]+x[1])/2)\n",
    "        ages_values.append(x)\n",
    "    else:\n",
    "        ages_values.append(int(x))\n",
    "        \n",
    "# Age map to use for regression.\n",
    "ages_map = {}\n",
    "for key, value in zip(ages_keys, ages_values):\n",
    "    ages_map[key] = value\n",
    "    \n",
    "df[\"age\"] = df[\"age\"].map(ages_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3ab9c8-96f7-40bf-82bd-835acb7144e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will later make the model predict a non-binary category\n",
    "\n",
    "df = df[df[\"gender\"] != \"u\"]\n",
    "df[\"gender\"] = df[\"gender\"].apply(lambda x: 1 if x == \"m\" else 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fc16d45-ab9c-438e-a993-3574affdd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the path to the image\n",
    "\n",
    "df[\"face_id\"] = df[\"face_id\"].astype(str)\n",
    "df[\"path\"] = \"../Data/Raw/Archive/Faces/\"+df[\"user_id\"]+\"/coarse_tilt_aligned_face.\"+df[\"face_id\"]+\".\"+df[\"original_image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12212082-e9d8-4b1e-82cd-207cce97605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving the model for future use\n",
    "\n",
    "df.to_csv(\"../Data/Clean/Faces.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49f8b9-cf9a-4016-8f6a-aab4940d9a58",
   "metadata": {},
   "source": [
    "## X/y Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "304031d5-3a10-4bdd-9eb5-925427259245",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As we will do two models, we have two targets\n",
    "\n",
    "X = df[\"path\"]\n",
    "y_age = df[\"age\"]\n",
    "y_gender = df[\"gender\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d45d24-0aa5-4262-9581-17e3ea666858",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738fbdd9-b4d4-4075-98d2-74f1767d9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train and X_test is the same for both\n",
    "\n",
    "X_train, X_test, y_train_age, y_test_age = train_test_split(X, y_age, test_size = 0.22, random_state = 22)\n",
    "X_train, X_test, y_train_gender, y_test_gender = train_test_split(X, y_gender, test_size = 0.22, random_state = 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be3fdb-bd12-4634-a2f9-7a08468be006",
   "metadata": {},
   "source": [
    "## Image preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8d1da794-705d-4f4b-9aeb-c3f713fb312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image greyscaled, downscaled, size adjusted and transformed.\n",
    "\n",
    "def image_preprocess(path):\n",
    "    img = tf.io.read_file(np.array(path).ravel()[0])\n",
    "    img = tf.image.decode_jpeg(img, channels = 1, ratio = 2)\n",
    "    img = tf.image.resize(img, [64,64])\n",
    "    img = img / 255 # This part normalizes the image, scaling it down; 255 is the max, while 0 is the min\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e37a470d-f94c-4cd1-b28e-e264eb2c32f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data that will be fed to the model, needs to be np.array\n",
    "\n",
    "X_train_images = np.array([image_preprocess(path) for path in X_train])\n",
    "X_test_images = np.array([image_preprocess(path) for path in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8f6184-2696-4e66-90dc-362dd25878a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Saving both arrays for future use\n",
    "\n",
    "filename = \"../Data/Clean/X_train_images.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(X_train_images, file)\n",
    "    \n",
    "filename = \"../Data/Clean/X_test_images.pkl\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    pickle.dump(X_test_images, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62077594-80b7-4021-afd1-2ac9a56b6bd2",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84b3ac-0553-4638-99c4-2b1f6b9f53a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f8f2a5-6ebf-4126-b6ab-bc45c268a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_age = Sequential()\n",
    "# First layer needs as many nodes as inputs\n",
    "model_age.add(Conv2D(64,(2,2), activation = \"relu\", input_shape = (64,64,1)))\n",
    "model_age.add(MaxPool2D((2,2)))\n",
    "model_age.add(Conv2D(64,(2,2), activation = \"relu\"))\n",
    "model_age.add(MaxPool2D((2,2)))\n",
    "model_age.add(Conv2D(64,(2,2), activation = \"relu\"))\n",
    "model_age.add(MaxPool2D((2,2)))\n",
    "model_age.add(Flatten())\n",
    "model_age.add(Dense(64, activation = \"relu\"))\n",
    "model_age.add(Dense(1, activation = \"relu\"))\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.01)\n",
    "model_age.compile(optimizer = opt,\n",
    "              loss = \"mse\",\n",
    "              metrics = [\"mae\", \"mse\", \"mape\"])\n",
    "model_age.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6fe81-0446-45d8-a25d-86ec255036bc",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1766e42-9af3-4590-885f-a7fde8291e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This changes the learning rate based on epochs\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr - (lr/(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bfb87ea-b313-4386-b091-3bde0b6818e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patience\n",
    "    \n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "# Checkpoint\n",
    "\n",
    "checkpoint_path = '../Models/Age_NN6.hdf5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_freq='epoch',\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Learning rate\n",
    "\n",
    "schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "049d4423-a1b3-4b88-91ef-f58d6c487289",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_age.fit(\n",
    "    X_train_images, y_train_age,\n",
    "    epochs=100,\n",
    "    validation_data = (X_test_images, y_test_age),\n",
    "    batch_size=128,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stop, checkpoint, schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef71a1-57e7-411e-bada-157261d54d27",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f819187-4ca0-40b1-a310-15e197db2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_age.predict(X_train_images)\n",
    "y_test_pred  = model_age.predict(X_test_images)\n",
    "\n",
    "display(mean_absolute_error(y_train_age,y_train_pred))\n",
    "display(mean_absolute_error(y_test_age,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea186d-0cae-4943-8520-b3146672ed21",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3935bd8a-252e-40bc-84fb-0f1c1e9e3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gender = Sequential()\n",
    "# First layer needs as many nodes as inputs\n",
    "model_gender.add(Conv2D(64,(2,2), activation = \"relu\", input_shape = (64,64,1)))\n",
    "model_gender.add(MaxPool2D((2,2)))\n",
    "model_gender.add(Conv2D(64,(2,2), activation = \"relu\"))\n",
    "model_gender.add(MaxPool2D((2,2)))\n",
    "model_gender.add(Conv2D(64,(2,2), activation = \"relu\"))\n",
    "model_gender.add(MaxPool2D((2,2)))\n",
    "model_gender.add(Flatten())\n",
    "model_gender.add(Dense(64, activation = \"relu\"))\n",
    "model_gender.add(Dense(1, activation = \"sigmoid\"))\n",
    "model_gender.compile(optimizer = \"adam\",\n",
    "              loss = \"binary_crossentropy\",\n",
    "              metrics = \"accuracy\")\n",
    "model_gender.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b70d4-57e5-414c-8a40-18e2a2b852fa",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d86da9f-df37-42e5-936c-746c48c855d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "checkpoint_path = '../Models/Gender_NN16.hdf5'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_freq='epoch',\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3590e5d-fe2a-4cfa-a93d-30ae5d746613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "\n",
      "Epoch 1: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 123s - loss: 0.9337 - accuracy: 0.5584 - val_loss: 0.6604 - val_accuracy: 0.6083 - lr: 0.0030 - 123s/epoch - 1s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 110s - loss: 0.6526 - accuracy: 0.6080 - val_loss: 0.6483 - val_accuracy: 0.6156 - lr: 0.0030 - 110s/epoch - 1s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 111s - loss: 0.6484 - accuracy: 0.6167 - val_loss: 0.6288 - val_accuracy: 0.6448 - lr: 0.0030 - 111s/epoch - 1s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 117s - loss: 0.6615 - accuracy: 0.6198 - val_loss: 0.6834 - val_accuracy: 0.5604 - lr: 0.0030 - 117s/epoch - 1s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 104s - loss: 0.6761 - accuracy: 0.5732 - val_loss: 0.6611 - val_accuracy: 0.6182 - lr: 0.0030 - 104s/epoch - 971ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 106s - loss: 0.6541 - accuracy: 0.6120 - val_loss: 0.6736 - val_accuracy: 0.6297 - lr: 0.0024 - 106s/epoch - 992ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 105s - loss: 0.6506 - accuracy: 0.6213 - val_loss: 0.6520 - val_accuracy: 0.6237 - lr: 0.0020 - 105s/epoch - 977ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: saving model to ../Models\\Gender_NN16.hdf5\n",
      "107/107 - 109s - loss: 0.6428 - accuracy: 0.6236 - val_loss: 0.6438 - val_accuracy: 0.6266 - lr: 0.0017 - 109s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "history = model_gender.fit(\n",
    "    X_train_images, y_train_gender,\n",
    "    epochs=50,\n",
    "    validation_data = (X_test_images, y_test_gender),\n",
    "    batch_size=128,\n",
    "    verbose=2,\n",
    "    callbacks=[early_stop, checkpoint, schedule]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ad717-bef0-4bd5-8506-2f431687bd32",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4afb48-7247-42af-b9b5-a181511d5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model_gender.predict(X_train_images)\n",
    "y_test_pred  = model_gender.predict(X_test_images)\n",
    "\n",
    "\n",
    "# This is for binary_crossentropy (1 neuron final output)\n",
    "y_train_pred2 = [int(round(y_train_pred[x][0],0)) for x in range(len(y_train_pred))]\n",
    "y_test_pred2 = [int(round(y_test_pred[x][0],0)) for x in range(len(y_test_pred))]\n",
    "\n",
    "\n",
    "# This is for sparse_categorical_crossestropy (2 neurons final output)\n",
    "# y_train_pred2 = np.argmax(y_train_pred, axis=1).reshape(-1,1)\n",
    "# y_test_pred2 = np.argmax(y_test_pred, axis=1).reshape(-1,1)\n",
    "\n",
    "print(\"Kappa score:\",cohen_kappa_score(y_train_gender, y_train_pred2))\n",
    "print(classification_report(y_train_gender, y_train_pred2, zero_division = True))\n",
    "print(\"Kappa score:\",cohen_kappa_score(y_test_gender, y_test_pred2))\n",
    "print(classification_report(y_test_gender, y_test_pred2, zero_division = True))\n",
    "\n",
    "\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/20Female.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/25Female1.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/25Female2.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/50Female1.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/50Female2.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/50Female3.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/50Female4.jpg\")])))\n",
    "\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/30Male1.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/30Male2.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/30Male3.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/40Male.jpg\")])))\n",
    "display(model_gender.predict(np.array([image_preprocess(\"../Data/Test/50Male.jpg\")])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4ce2f-4bf1-4162-adc8-fb23e579d3f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf42643-f704-4df0-8713-5191bdadcbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_age = load_model(\"../Models/Age_NN3.hdf5\")\n",
    "model_gender = load_model(\"../Models/Gender_NN3.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10bacc0a-dde1-412c-a154-27cdcd1d9214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3190d9f8-b16e-454e-b8c2-3645d557df3e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_augmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Models/Augmentation.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Programs\\Anaconda3\\envs\\Character-Selection\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\Programs\\Anaconda3\\envs\\Character-Selection\\lib\\site-packages\\keras\\engine\\training.py:3467\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   3458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3459\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m   3460\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m!=\u001b[39m Model\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3465\u001b[0m     \u001b[38;5;66;03m# Also make sure to exclude Model class itself which has build()\u001b[39;00m\n\u001b[0;32m   3466\u001b[0m     \u001b[38;5;66;03m# defined.\u001b[39;00m\n\u001b[1;32m-> 3467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m have not yet been \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3469\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3470\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights are created when the Model is first called on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3471\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3472\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential_3 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "data_augmentation.save(\"../Models/Augmentation.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf7b3b7-47b3-441c-ad62-d4179e5936ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x00000252A43B2D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x00000252A43B2F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "image1 = image_preprocess(\"../Data/Test/25Female1.jpg\")\n",
    "images.append(np.array([image1]))\n",
    "i = 0\n",
    "while i < 9:\n",
    "    image = image_preprocess(\"../Data/Test/25Female1.jpg\")\n",
    "    image = tf.cast(tf.expand_dims(image, 0), tf.float32)\n",
    "    image = data_augmentation(image)\n",
    "    images.append(image)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cfd31ca-080d-4ba8-8d92-496380acf5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m predictions_female \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m predictions_male \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m \u001b[43mimages\u001b[49m:\n\u001b[0;32m      5\u001b[0m     predictions_age\u001b[38;5;241m.\u001b[39mappend(model_age\u001b[38;5;241m.\u001b[39mpredict(image)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      6\u001b[0m     predictions_female\u001b[38;5;241m.\u001b[39mappend(model_gender\u001b[38;5;241m.\u001b[39mpredict(image)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_age = []\n",
    "predictions_female = []\n",
    "predictions_male = []\n",
    "for image in images:\n",
    "    predictions_age.append(model_age.predict(image)[0])\n",
    "    predictions_female.append(model_gender.predict(image)[0][0])\n",
    "    predictions_male.append(model_gender.predict(image)[0][1])\n",
    "    \n",
    "print(np.mean(predictions_age))\n",
    "print(np.mean(predictions_female))\n",
    "print(np.mean(predictions_male))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1c5338e-f775-4dd6-9573-ae5853a19d36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.86575  , 0.0824468]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gender.predict(np.array([image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f7cab1d-2531-4b27-a6fd-4f0537cfb83a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Age: 13.148493766784668, Female: 0.8690377473831177, Male: 0.12919019162654877\n"
     ]
    }
   ],
   "source": [
    "image = image_preprocess(\"../Data/Test/25Female1.jpg\")\n",
    "age, female, male = image_augmentation(image, model_age, model_gender)\n",
    "\n",
    "print(\"Age: {}, Female: {}, Male: {}\".format(age, female, male))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
